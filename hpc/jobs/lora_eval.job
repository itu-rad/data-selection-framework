#!/bin/bash
#SBATCH --job-name=lora_eval    # Job name
#SBATCH --output=hpc/output/lora_eval.out       # Output file
#SBATCH --cpus-per-task=8                  # CPU cores per task (adjust based on your needs)
#SBATCH --gres=gpu:v100                     # Request certain GPU
#SBATCH --mem=64G
#SBATCH --time=72:00:00                         # Run for up to 3 days
#SBATCH --partition=acltr                       # Run on GPU queue
 
echo "Running on $(hostname):"
nvidia-smi
 
# Evaluate base and finetuned model, remember to use last epoch in config
echo "Starting evaluation of model(s)..."
tune run less/recipe/eval.py --config hpc/configs/FullSampler_lora_e0.yaml
tune run less/recipe/eval.py --config hpc/configs/FullSampler_lora_e1.yaml
tune run less/recipe/eval.py --config hpc/configs/FullSampler_lora_e2.yaml
tune run less/recipe/eval.py --config hpc/configs/FullSampler_lora_e3.yaml