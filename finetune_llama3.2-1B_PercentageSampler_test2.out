Running on cn3.hpc.itu.dk:
Mon Mar 31 17:01:03 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  Tesla V100-PCIE-32GB           On  |   00000000:3F:00.0 Off |                    0 |
| N/A   47C    P0             28W /  250W |       1MiB /  32768MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
/home/nikch/.conda/envs/selection/lib/python3.12/site-packages/radt/schedule/schedule.py:514: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'A' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.
  df_workload.loc[i, "Number"] = ascii_uppercase[
[33mExecuting command - ['echo quit | nvidia-cuda-mps-control'][0m
Cannot find MPS control daemon process
[33mExecuting command - ['nvidia-smi', '-L'][0m
[33mExecuting command - ['dcgmi', 'group', '-l'][0m
[33mExecuting command - ['dcgmi', 'group', '-d', '10'][0m
[33mExecuting command - ['dcgmi', 'group', '-c', 'mldnn_0'][0m
[33mExecuting command - ['dcgmi', 'group', '-g', '11', '-a', '0'][0m
[33mRUNNING WORKLOAD: 137+0[0m
[31m[RUN 0]:            [0m context: 0-31-0-{'MLFLOW_EXPERIMENT_ID': '137', 'CUDA_VISIBLE_DEVICES': 'GPU-a5dc6d38-83c6-699e-b520-388da8593f59', 'RADT_DCGMI_GROUP': '11', 'SMI_GPU_ID': '0', 'RADT_MAX_EPOCH': '5', 'RADT_MAX_TIME': '172800', 'RADT_MANUAL_MODE': 'True', 'ps': 'False', 'smi': 'False', 'dcgmi': 'False', 'top': 'False', 'iostat': 'False', 'free': 'False', 'RADT_LISTENER_SMI': 'True', 'RADT_LISTENER_TOP': 'True', 'RADT_LISTENER_DCGMI': 'True', 'RADT_LISTENER_IOSTAT': 'True', 'RADT_LISTENER_FREE': 'True'}-['mlflow', 'run', '/home/nikch/data-selection-framework', '--env-manager=local', '-P', 'letter=0', '-P', 'workload=0', '-P', 'listeners=smi+top+dcgmi+iostat+free', '-P', 'file=tune.py', '-P', 'params="-"', '-P', 'workload_listener=']-/home/nikch/data-selection-framework
[31m[RUN 0]:            [0m 2025/03/31 17:01:08 INFO mlflow.projects.utils: === Created directory /tmp/tmp3gorkhc9 for downloading remote URIs passed to arguments of type 'path' ===
[31m[RUN 0]:            [0m 2025/03/31 17:01:08 INFO mlflow.projects.backend.local: === Running command 'python -m radt run -l smi+top+dcgmi+iostat+free -c tune.py -p "run recipe/test_lora_finetune.py --config less/warmup_train.yaml"
[31m[RUN 0]:            [0m ' in run with ID 'db651d3924c64dd5a2da84649d99c654' === 
[31m[RUN 0]:            [0m MAPPED TO db651d3924c64dd5a2da84649d99c654
[31m[RUN 0]:            [0m Running LoRAFinetuneRecipeSingleDevice with resolved config:
[31m[RUN 0]:            [0m 
[31m[RUN 0]:            [0m batch_size: 8
[31m[RUN 0]:            [0m checkpointer:
[31m[RUN 0]:            [0m   _component_: torchtune.training.FullModelHFCheckpointer
[31m[RUN 0]:            [0m   checkpoint_dir: model_cache/downloaded_models/Llama-3.2-1B-Instruct/
[31m[RUN 0]:            [0m   checkpoint_files:
[31m[RUN 0]:            [0m   - model.safetensors
[31m[RUN 0]:            [0m   model_type: LLAMA3_2
[31m[RUN 0]:            [0m   output_dir: model_cache/less/warmup_models/llama3_2_1B
[31m[RUN 0]:            [0m   recipe_checkpoint: null
[31m[RUN 0]:            [0m clip_grad_norm: null
[31m[RUN 0]:            [0m compile: false
[31m[RUN 0]:            [0m dataset:
[31m[RUN 0]:            [0m   _component_: torchtune.datasets.alpaca_cleaned_dataset
[31m[RUN 0]:            [0m   packed: false
[31m[RUN 0]:            [0m device: cuda
[31m[RUN 0]:            [0m dtype: bf16
[31m[RUN 0]:            [0m enable_activation_checkpointing: true
[31m[RUN 0]:            [0m enable_activation_offloading: true
[31m[RUN 0]:            [0m epochs: 1
[31m[RUN 0]:            [0m gradient_accumulation_steps: 32
[31m[RUN 0]:            [0m log_every_n_steps: 1
[31m[RUN 0]:            [0m log_peak_memory_stats: true
[31m[RUN 0]:            [0m loss:
[31m[RUN 0]:            [0m   _component_: torchtune.modules.loss.CEWithChunkedOutputLoss
[31m[RUN 0]:            [0m lr_scheduler:
[31m[RUN 0]:            [0m   _component_: torchtune.training.lr_schedulers.get_cosine_schedule_with_warmup
[31m[RUN 0]:            [0m   num_warmup_steps: 100
[31m[RUN 0]:            [0m max_steps_per_epoch: null
[31m[RUN 0]:            [0m metric_logger:
[31m[RUN 0]:            [0m   _component_: torchtune.training.metric_logging.DiskLogger
[31m[RUN 0]:            [0m   log_dir: model_cache/less/warmup_models/llama3_2_1B/logs
[31m[RUN 0]:            [0m model:
[31m[RUN 0]:            [0m   _component_: torchtune.models.llama3_2.lora_llama3_2_1b
[31m[RUN 0]:            [0m   apply_lora_to_mlp: true
[31m[RUN 0]:            [0m   lora_alpha: 512
[31m[RUN 0]:            [0m   lora_attn_modules:
[31m[RUN 0]:            [0m   - q_proj
[31m[RUN 0]:            [0m   - k_proj
[31m[RUN 0]:            [0m   - v_proj
[31m[RUN 0]:            [0m   - output_proj
[31m[RUN 0]:            [0m   lora_dropout: 0.1
[31m[RUN 0]:            [0m   lora_rank: 128
[31m[RUN 0]:            [0m optimizer:
[31m[RUN 0]:            [0m   _component_: torch.optim.AdamW
[31m[RUN 0]:            [0m   fused: true
[31m[RUN 0]:            [0m   lr: 2.0e-05
[31m[RUN 0]:            [0m   weight_decay: 0.0
[31m[RUN 0]:            [0m output_dir: model_cache/less/warmup_models/llama3_2_1B
[31m[RUN 0]:            [0m profiler:
[31m[RUN 0]:            [0m   _component_: torchtune.training.setup_torch_profiler
[31m[RUN 0]:            [0m   active_steps: 2
[31m[RUN 0]:            [0m   cpu: true
[31m[RUN 0]:            [0m   cuda: true
[31m[RUN 0]:            [0m   enabled: false
[31m[RUN 0]:            [0m   num_cycles: 1
[31m[RUN 0]:            [0m   output_dir: model_cache/less/warmup_models/llama3_2_1B/profiling_outputs
[31m[RUN 0]:            [0m   profile_memory: false
[31m[RUN 0]:            [0m   record_shapes: true
[31m[RUN 0]:            [0m   wait_steps: 5
[31m[RUN 0]:            [0m   warmup_steps: 3
[31m[RUN 0]:            [0m   with_flops: false
[31m[RUN 0]:            [0m   with_stack: false
[31m[RUN 0]:            [0m resume_from_checkpoint: false
[31m[RUN 0]:            [0m sampler:
[31m[RUN 0]:            [0m   _component_: selection.PercentageBasedSampler
[31m[RUN 0]:            [0m   percentage: 0.05
[31m[RUN 0]:            [0m save_adapter_weights_only: false
[31m[RUN 0]:            [0m seed: null
[31m[RUN 0]:            [0m shuffle: true
[31m[RUN 0]:            [0m tokenizer:
[31m[RUN 0]:            [0m   _component_: torchtune.models.llama3.llama3_tokenizer
[31m[RUN 0]:            [0m   max_seq_len: null
[31m[RUN 0]:            [0m   path: model_cache/downloaded_models/Llama-3.2-1B-Instruct/original/tokenizer.model
[31m[RUN 0]:            [0m 
[31m[RUN 0]:            [0m Setting manual seed to local seed 1139578390. Local seed is seed + rank = 1139578390 + 0
[31m[RUN 0]:            [0m Model is initialized with precision torch.bfloat16.
[31m[RUN 0]:            [0m Memory stats after model init:
[31m[RUN 0]:            [0m 	GPU peak memory allocation: 2.50 GiB
[31m[RUN 0]:            [0m 	GPU peak memory reserved: 2.52 GiB
[31m[RUN 0]:            [0m 	GPU peak memory active: 2.50 GiB
[31m[RUN 0]:            [0m Tokenizer is initialized from file.
[31m[RUN 0]:            [0m Optimizer and loss are initialized.
[31m[RUN 0]:            [0m Loss is initialized.
[31m[RUN 0]:            [0m Dataset and SelectiveSampler are initialized.
[31m[RUN 0]:            [0m Learning rate scheduler is initialized.
[31m[RUN 0]:            [0m  Profiling disabled.
[31m[RUN 0]:            [0m  Profiler config after instantiation: {'enabled': False}
[31m[RUN 0]:            [0m Failed to log parameter: params run recipe/test_lora_finetune.py --config less/warmup_train.yaml
[31m[RUN 0]:            [0m Starting test_lora_finetune setup()
[31m[RUN 0]:            [0m Writing logs to model_cache/less/warmup_models/llama3_2_1B/logs/log_1743433279.txt
[31m[RUN 0]:            [0m starting test_lora_finetune train()
[31m[RUN 0]:            [0m Process IOstatThread-4:
[31m[RUN 0]:            [0m 2025/03/31 17:01:29 INFO mlflow.bedrock: Enabled auto-tracing for Bedrock. Note that MLflow can only trace boto3 service clients that are created after this call. If you have already created one, please recreate the client by calling `boto3.client`.
[31m[RUN 0]:            [0m Traceback (most recent call last):
[31m[RUN 0]:            [0m   File "/home/nikch/.conda/envs/selection/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[31m[RUN 0]:            [0m     self.run()
[31m[RUN 0]:            [0m   File "/home/nikch/.conda/envs/selection/lib/python3.12/site-packages/radt/run/listeners/iostat_listener.py", line 17, in run
[31m[RUN 0]:            [0m     ps = subprocess.Popen(
[31m[RUN 0]:            [0m          ^^^^^^^^^^^^^^^^^
[31m[RUN 0]:            [0m   File "/home/nikch/.conda/envs/selection/lib/python3.12/subprocess.py", line 1028, in __init__
[31m[RUN 0]:            [0m     self._execute_child(args, executable, preexec_fn, close_fds,
[31m[RUN 0]:            [0m   File "/home/nikch/.conda/envs/selection/lib/python3.12/subprocess.py", line 1963, in _execute_child
[31m[RUN 0]:            [0m     raise child_exception_type(errno_num, err_msg, err_filename)
[31m[RUN 0]:            [0m FileNotFoundError: [Errno 2] No such file or directory: 'iostat'
[31m[RUN 0]:            [0m 2025/03/31 17:01:29 INFO mlflow.tracking.fluent: Autologging successfully enabled for boto3.
[31m[RUN 0]:            [0m 
[31m[RUN 0]:            [0m   0%|          | 0/10 [00:00<?, ?it/s]
[31m[RUN 0]:            [0m  10%|█         | 1/10 [01:45<15:47, 105.29s/it]
[31m[RUN 0]:            [0m 1|1|Loss: 1.9925460815429688:  10%|█         | 1/10 [01:45<15:47, 105.29s/it]
[31m[RUN 0]:            [0m 1|1|Loss: 1.9925460815429688:  20%|██        | 2/10 [03:37<14:34, 109.27s/it]
[31m[RUN 0]:            [0m 1|2|Loss: 2.0221447944641113:  20%|██        | 2/10 [03:37<14:34, 109.27s/it]Starting checkpoint save...
[31m[RUN 0]:            [0m [31m2025/03/31 17:05:34 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.[0m
[31m[RUN 0]:            [0m Model checkpoint of size 2.30 GiB saved to model_cache/less/warmup_models/llama3_2_1B/epoch_0/ft-model-00001-of-00001.safetensors
[31m[RUN 0]:            [0m INFO:torchtune.utils._logging:Model checkpoint of size 2.30 GiB saved to model_cache/less/warmup_models/llama3_2_1B/epoch_0/ft-model-00001-of-00001.safetensors
[31m[RUN 0]:            [0m Adapter checkpoint of size 0.17 GiB saved to model_cache/less/warmup_models/llama3_2_1B/epoch_0/adapter_model.pt
[31m[RUN 0]:            [0m INFO:torchtune.utils._logging:Adapter checkpoint of size 0.17 GiB saved to model_cache/less/warmup_models/llama3_2_1B/epoch_0/adapter_model.pt
[31m[RUN 0]:            [0m Adapter checkpoint of size 0.17 GiB saved to model_cache/less/warmup_models/llama3_2_1B/epoch_0/adapter_model.safetensors
[31m[RUN 0]:            [0m INFO:torchtune.utils._logging:Adapter checkpoint of size 0.17 GiB saved to model_cache/less/warmup_models/llama3_2_1B/epoch_0/adapter_model.safetensors
[31m[RUN 0]:            [0m Adapter checkpoint of size 0.00 GiB saved to model_cache/less/warmup_models/llama3_2_1B/epoch_0/adapter_config.json
[31m[RUN 0]:            [0m INFO:torchtune.utils._logging:Adapter checkpoint of size 0.00 GiB saved to model_cache/less/warmup_models/llama3_2_1B/epoch_0/adapter_config.json
[31m[RUN 0]:            [0m Saving final epoch checkpoint.
[31m[RUN 0]:            [0m INFO:torchtune.utils._logging:Saving final epoch checkpoint.
[31m[RUN 0]:            [0m The full model checkpoint, including all weights and configurations, has been saved successfully.You can now use this checkpoint for further training or inference.
[31m[RUN 0]:            [0m INFO:torchtune.utils._logging:The full model checkpoint, including all weights and configurations, has been saved successfully.You can now use this checkpoint for further training or inference.
[31m[RUN 0]:            [0m Checkpoint saved in 92.97 seconds.
[31m[RUN 0]:            [0m INFO:torchtune.utils._logging:Checkpoint saved in 92.97 seconds.
[31m[RUN 0]:            [0m 
[31m[RUN 0]:            [0m 1|2|Loss: 2.0221447944641113:  20%|██        | 2/10 [05:10<20:41, 155.19s/it]
[31m[RUN 0]:            [0m 🏃 View run (0 0) capable-carp-365 at: https://res17.itu.dk/#/experiments/137/runs/db651d3924c64dd5a2da84649d99c654
[31m[RUN 0]:            [0m 🧪 View experiment at: https://res17.itu.dk/#/experiments/137
[31m[RUN 0]:            [0m starting test_lora_finetune cleanup()
[31m[RUN 0]:            [0m 2025/03/31 17:06:41 INFO mlflow.projects: === Run (ID 'db651d3924c64dd5a2da84649d99c654') succeeded ===
[31m[RUN 0]:            [0m 🏃 View run (0 0) capable-carp-365 at: https://res17.itu.dk/#/experiments/137/runs/db651d3924c64dd5a2da84649d99c654
[31m[RUN 0]:            [0m 🧪 View experiment at: https://res17.itu.dk/#/experiments/137
[33mSending logs to server.[0m
[33mExecuting command - ['echo quit | nvidia-cuda-mps-control'][0m
Cannot find MPS control daemon process
run_experiment.sh: Starting evaluation of base model...
Running EleutherEvalRecipe with resolved config:

batch_size: 8
checkpointer:
  _component_: torchtune.training.FullModelHFCheckpointer
  checkpoint_dir: model_cache/finetuned_models/torchtune/llama3_2_1B/lora_single_device/epoch_3/
  checkpoint_files:
  - ft-model-00001-of-00001.safetensors
  model_type: LLAMA3_2
  output_dir: ./
device: cuda
dtype: bf16
enable_kv_cache: true
limit: null
max_seq_length: 4096
model:
  _component_: torchtune.models.llama3_2.llama3_2_1b
output_dir: ./
quantizer: null
seed: 1234
tasks:
- truthfulqa_mc2
tokenizer:
  _component_: torchtune.models.llama3.llama3_tokenizer
  max_seq_len: null
  path: model_cache/downloaded_models/Llama-3.2-1B-Instruct/original/tokenizer.model

Traceback (most recent call last):
  File "/home/nikch/.conda/envs/selection/bin/tune", line 8, in <module>
    sys.exit(main())
             ^^^^^^
  File "/home/nikch/.conda/envs/selection/lib/python3.12/site-packages/torchtune/_cli/tune.py", line 49, in main
    parser.run(args)
  File "/home/nikch/.conda/envs/selection/lib/python3.12/site-packages/torchtune/_cli/tune.py", line 43, in run
    args.func(args)
  File "/home/nikch/.conda/envs/selection/lib/python3.12/site-packages/torchtune/_cli/run.py", line 214, in _run_cmd
    self._run_single_device(args, is_builtin=is_builtin)
  File "/home/nikch/.conda/envs/selection/lib/python3.12/site-packages/torchtune/_cli/run.py", line 111, in _run_single_device
    runpy.run_module(str(args.recipe), run_name="__main__")
  File "<frozen runpy>", line 229, in run_module
  File "<frozen runpy>", line 88, in _run_code
  File "/home/nikch/data-selection-framework/recipe/eval.py", line 567, in <module>
    sys.exit(recipe_main())
             ^^^^^^^^^^^^^
  File "/home/nikch/.conda/envs/selection/lib/python3.12/site-packages/torchtune/config/_parse.py", line 99, in wrapper
    sys.exit(recipe_main(conf))
             ^^^^^^^^^^^^^^^^^
  File "/home/nikch/data-selection-framework/recipe/eval.py", line 562, in recipe_main
    recipe.setup(cfg=cfg)
  File "/home/nikch/data-selection-framework/recipe/eval.py", line 469, in setup
    checkpointer = config.instantiate(cfg.checkpointer)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/nikch/.conda/envs/selection/lib/python3.12/site-packages/torchtune/config/_instantiate.py", line 112, in instantiate
    return _instantiate_node(OmegaConf.to_object(config), *args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/nikch/.conda/envs/selection/lib/python3.12/site-packages/torchtune/config/_instantiate.py", line 33, in _instantiate_node
    return _create_component(_component_, args, kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/nikch/.conda/envs/selection/lib/python3.12/site-packages/torchtune/config/_instantiate.py", line 22, in _create_component
    return _component_(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/nikch/.conda/envs/selection/lib/python3.12/site-packages/torchtune/training/checkpointing/_checkpointer.py", line 409, in __init__
    Path.joinpath(self._checkpoint_dir, "config.json").read_text()
  File "/home/nikch/.conda/envs/selection/lib/python3.12/pathlib.py", line 1027, in read_text
    with self.open(mode='r', encoding=encoding, errors=errors) as f:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/nikch/.conda/envs/selection/lib/python3.12/pathlib.py", line 1013, in open
    return io.open(self, mode, buffering, encoding, errors, newline)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: 'model_cache/finetuned_models/torchtune/llama3_2_1B/lora_single_device/epoch_3/config.json'
