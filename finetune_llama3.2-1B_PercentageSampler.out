Running on cn3.hpc.itu.dk:
Mon Mar 31 13:11:49 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  Tesla V100-PCIE-32GB           On  |   00000000:3F:00.0 Off |                    0 |
| N/A   28C    P0             25W /  250W |       1MiB /  32768MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
run_experiment.sh: Starting model download...
Ignoring files matching the following patterns: original/consolidated.00.pth
Successfully downloaded model repo and wrote to the following locations:
/home/nikch/data-selection-framework/model_cache/downloaded_models/Llama-3.2-1B-Instruct/.cache
/home/nikch/data-selection-framework/model_cache/downloaded_models/Llama-3.2-1B-Instruct/.gitattributes
/home/nikch/data-selection-framework/model_cache/downloaded_models/Llama-3.2-1B-Instruct/LICENSE.txt
/home/nikch/data-selection-framework/model_cache/downloaded_models/Llama-3.2-1B-Instruct/README.md
/home/nikch/data-selection-framework/model_cache/downloaded_models/Llama-3.2-1B-Instruct/USE_POLICY.md
/home/nikch/data-selection-framework/model_cache/downloaded_models/Llama-3.2-1B-Instruct/config.json
/home/nikch/data-selection-framework/model_cache/downloaded_models/Llama-3.2-1B-Instruct/generation_config.json
/home/nikch/data-selection-framework/model_cache/downloaded_models/Llama-3.2-1B-Instruct/model.safetensors
/home/nikch/data-selection-framework/model_cache/downloaded_models/Llama-3.2-1B-Instruct/original
/home/nikch/data-selection-framework/model_cache/downloaded_models/Llama-3.2-1B-Instruct/special_tokens_map.json
/home/nikch/data-selection-framework/model_cache/downloaded_models/Llama-3.2-1B-Instruct/tokenizer.json
/home/nikch/data-selection-framework/model_cache/downloaded_models/Llama-3.2-1B-Instruct/tokenizer_config.json
/home/nikch/data-selection-framework/model_cache/downloaded_models/Llama-3.2-1B-Instruct/original_repo_id.json
run_experiment.sh: Starting fine-tuning with config...
/home/nikch/.conda/envs/selection/lib/python3.12/site-packages/radt/schedule/schedule.py:514: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'A' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.
  df_workload.loc[i, "Number"] = ascii_uppercase[
[33mExecuting command - ['echo quit | nvidia-cuda-mps-control'][0m
Cannot find MPS control daemon process
[33mExecuting command - ['nvidia-smi', '-L'][0m
[33mExecuting command - ['dcgmi', 'group', '-l'][0m
[33mExecuting command - ['dcgmi', 'group', '-d', '5'][0m
[33mExecuting command - ['dcgmi', 'group', '-c', 'mldnn_0'][0m
[33mExecuting command - ['dcgmi', 'group', '-g', '6', '-a', '0'][0m
[33mRUNNING WORKLOAD: 137+0[0m
[31m[RUN 0]:            [0m context: 0-31-0-{'MLFLOW_EXPERIMENT_ID': '137', 'CUDA_VISIBLE_DEVICES': 'GPU-a5dc6d38-83c6-699e-b520-388da8593f59', 'RADT_DCGMI_GROUP': '6', 'SMI_GPU_ID': '0', 'RADT_MAX_EPOCH': '5', 'RADT_MAX_TIME': '172800', 'RADT_MANUAL_MODE': 'True', 'ps': 'False', 'smi': 'False', 'dcgmi': 'False', 'top': 'False', 'iostat': 'False', 'free': 'False', 'RADT_LISTENER_SMI': 'True', 'RADT_LISTENER_TOP': 'True', 'RADT_LISTENER_DCGMI': 'True', 'RADT_LISTENER_IOSTAT': 'True', 'RADT_LISTENER_FREE': 'True'}-['mlflow', 'run', '/home/nikch/data-selection-framework', '--env-manager=local', '-P', 'letter=0', '-P', 'workload=0', '-P', 'listeners=smi+top+dcgmi+iostat+free', '-P', 'file=tune.py', '-P', 'params="-"', '-P', 'workload_listener=']-/home/nikch/data-selection-framework
[31m[RUN 0]:            [0m 2025/03/31 13:12:07 INFO mlflow.projects.utils: === Created directory /tmp/tmp3efrtqcm for downloading remote URIs passed to arguments of type 'path' ===
[31m[RUN 0]:            [0m 2025/03/31 13:12:07 INFO mlflow.projects.backend.local: === Running command 'python -m radt run -l smi+top+dcgmi+iostat+free -c tune.py -p "run recipe/test_full_finetune.py --config config/llama3_2/1b_full/train.yaml"
[31m[RUN 0]:            [0m ' in run with ID '5557f929577c4e9aadc068542c0afc5e' === 
[31m[RUN 0]:            [0m MAPPED TO 5557f929577c4e9aadc068542c0afc5e
[31m[RUN 0]:            [0m Running FullFinetuneRecipeSingleDevice with resolved config:
[31m[RUN 0]:            [0m 
[31m[RUN 0]:            [0m batch_size: 4
[31m[RUN 0]:            [0m checkpointer:
[31m[RUN 0]:            [0m   _component_: torchtune.training.FullModelHFCheckpointer
[31m[RUN 0]:            [0m   checkpoint_dir: model_cache/downloaded_models/Llama-3.2-1B-Instruct/
[31m[RUN 0]:            [0m   checkpoint_files:
[31m[RUN 0]:            [0m   - model.safetensors
[31m[RUN 0]:            [0m   model_type: LLAMA3_2
[31m[RUN 0]:            [0m   output_dir: model_cache/finetuned_models/torchtune/llama3_2_1B/full_single_device
[31m[RUN 0]:            [0m   recipe_checkpoint: null
[31m[RUN 0]:            [0m clip_grad_norm: null
[31m[RUN 0]:            [0m compile: false
[31m[RUN 0]:            [0m dataset:
[31m[RUN 0]:            [0m   _component_: torchtune.datasets.alpaca_dataset
[31m[RUN 0]:            [0m   packed: false
[31m[RUN 0]:            [0m device: cuda
[31m[RUN 0]:            [0m dtype: bf16
[31m[RUN 0]:            [0m enable_activation_checkpointing: false
[31m[RUN 0]:            [0m enable_activation_offloading: false
[31m[RUN 0]:            [0m epochs: 1
[31m[RUN 0]:            [0m gradient_accumulation_steps: 1
[31m[RUN 0]:            [0m log_every_n_steps: 1
[31m[RUN 0]:            [0m log_peak_memory_stats: true
[31m[RUN 0]:            [0m loss:
[31m[RUN 0]:            [0m   _component_: torchtune.modules.loss.CEWithChunkedOutputLoss
[31m[RUN 0]:            [0m max_steps_per_epoch: null
[31m[RUN 0]:            [0m metric_logger:
[31m[RUN 0]:            [0m   _component_: torchtune.training.metric_logging.DiskLogger
[31m[RUN 0]:            [0m   log_dir: model_cache/finetuned_models/torchtune/llama3_2_1B/full_single_device/logs
[31m[RUN 0]:            [0m model:
[31m[RUN 0]:            [0m   _component_: torchtune.models.llama3_2.llama3_2_1b
[31m[RUN 0]:            [0m optimizer:
[31m[RUN 0]:            [0m   _component_: bitsandbytes.optim.PagedAdamW8bit
[31m[RUN 0]:            [0m   lr: 2.0e-05
[31m[RUN 0]:            [0m optimizer_in_bwd: true
[31m[RUN 0]:            [0m output_dir: model_cache/finetuned_models/torchtune/llama3_2_1B/full_single_device
[31m[RUN 0]:            [0m profiler:
[31m[RUN 0]:            [0m   _component_: torchtune.training.setup_torch_profiler
[31m[RUN 0]:            [0m   active_steps: 2
[31m[RUN 0]:            [0m   cpu: true
[31m[RUN 0]:            [0m   cuda: true
[31m[RUN 0]:            [0m   enabled: false
[31m[RUN 0]:            [0m   num_cycles: 1
[31m[RUN 0]:            [0m   output_dir: model_cache/finetuned_models/torchtune/llama3_2_1B/full_single_device/profiling_outputs
[31m[RUN 0]:            [0m   profile_memory: false
[31m[RUN 0]:            [0m   record_shapes: true
[31m[RUN 0]:            [0m   wait_steps: 5
[31m[RUN 0]:            [0m   warmup_steps: 3
[31m[RUN 0]:            [0m   with_flops: false
[31m[RUN 0]:            [0m   with_stack: false
[31m[RUN 0]:            [0m resume_from_checkpoint: false
[31m[RUN 0]:            [0m sampler:
[31m[RUN 0]:            [0m   _component_: selection.PercentageBasedSampler
[31m[RUN 0]:            [0m   percentage: 0.05
[31m[RUN 0]:            [0m seed: null
[31m[RUN 0]:            [0m shuffle: true
[31m[RUN 0]:            [0m tokenizer:
[31m[RUN 0]:            [0m   _component_: torchtune.models.llama3.llama3_tokenizer
[31m[RUN 0]:            [0m   max_seq_len: null
[31m[RUN 0]:            [0m   path: model_cache/downloaded_models/Llama-3.2-1B-Instruct/original/tokenizer.model
[31m[RUN 0]:            [0m 
[31m[RUN 0]:            [0m Setting manual seed to local seed 3573454400. Local seed is seed + rank = 3573454400 + 0
[31m[RUN 0]:            [0m Model is initialized with precision torch.bfloat16.
[31m[RUN 0]:            [0m Memory stats after model init:
[31m[RUN 0]:            [0m 	GPU peak memory allocation: 2.33 GiB
[31m[RUN 0]:            [0m 	GPU peak memory reserved: 2.34 GiB
[31m[RUN 0]:            [0m 	GPU peak memory active: 2.33 GiB
[31m[RUN 0]:            [0m Tokenizer is initialized from file.
[31m[RUN 0]:            [0m In-backward optimizers are set up.
[31m[RUN 0]:            [0m Loss is initialized.
[31m[RUN 0]:            [0m Dataset and SelectiveSampler are initialized.
[31m[RUN 0]:            [0m No learning rate scheduler configured. Using constant learning rate.
[31m[RUN 0]:            [0m  Profiling disabled.
[31m[RUN 0]:            [0m  Profiler config after instantiation: {'enabled': False}
[31m[RUN 0]:            [0m Failed to log parameter: params run recipe/test_full_finetune.py --config config/llama3_2/1b_full/train.yaml
[31m[RUN 0]:            [0m Entering recipe_main in test_full_finetune
[31m[RUN 0]:            [0m Starting test_full_finetune setup()
[31m[RUN 0]:            [0m Writing logs to model_cache/finetuned_models/torchtune/llama3_2_1B/full_single_device/logs/log_1743419538.txt
[31m[RUN 0]:            [0m starting test_full_finetune train()
[31m[RUN 0]:            [0m Process IOstatThread-4:
[31m[RUN 0]:            [0m Traceback (most recent call last):
[31m[RUN 0]:            [0m   File "/home/nikch/.conda/envs/selection/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[31m[RUN 0]:            [0m     self.run()
[31m[RUN 0]:            [0m   File "/home/nikch/.conda/envs/selection/lib/python3.12/site-packages/radt/run/listeners/iostat_listener.py", line 17, in run
[31m[RUN 0]:            [0m     ps = subprocess.Popen(
[31m[RUN 0]:            [0m          ^^^^^^^^^^^^^^^^^
[31m[RUN 0]:            [0m   File "/home/nikch/.conda/envs/selection/lib/python3.12/subprocess.py", line 1028, in __init__
[31m[RUN 0]:            [0m     self._execute_child(args, executable, preexec_fn, close_fds,
[31m[RUN 0]:            [0m   File "/home/nikch/.conda/envs/selection/lib/python3.12/subprocess.py", line 1963, in _execute_child
[31m[RUN 0]:            [0m     raise child_exception_type(errno_num, err_msg, err_filename)
[31m[RUN 0]:            [0m FileNotFoundError: [Errno 2] No such file or directory: 'iostat'
[31m[RUN 0]:            [0m 2025/03/31 13:12:30 INFO mlflow.bedrock: Enabled auto-tracing for Bedrock. Note that MLflow can only trace boto3 service clients that are created after this call. If you have already created one, please recreate the client by calling `boto3.client`.
[31m[RUN 0]:            [0m 2025/03/31 13:12:30 INFO mlflow.tracking.fluent: Autologging successfully enabled for boto3.
[31m[RUN 0]:            [0m 
[31m[RUN 0]:            [0m   0%|          | 0/13000 [00:00<?, ?it/s]/home/nikch/.conda/envs/selection/lib/python3.12/site-packages/radt/schedule/schedule.py:514: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'A' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.
[31m[RUN 0]:            [0m   df_workload.loc[i, "Number"] = ascii_uppercase[
[31m[RUN 0]:            [0m [33mExecuting command - ['echo quit | nvidia-cuda-mps-control'][0m
[31m[RUN 0]:            [0m Cannot find MPS control daemon process
[31m[RUN 0]:            [0m [33mExecuting command - ['nvidia-smi', '-L'][0m
[31m[RUN 0]:            [0m [33mExecuting command - ['dcgmi', 'group', '-l'][0m
[31m[RUN 0]:            [0m [33mExecuting command - ['dcgmi', 'group', '-d', '6'][0m
[31m[RUN 0]:            [0m [33mExecuting command - ['dcgmi', 'group', '-c', 'mldnn_0'][0m
[31m[RUN 0]:            [0m [33mExecuting command - ['dcgmi', 'group', '-g', '7', '-a', '0'][0m
[31m[RUN 0]:            [0m [33mRUNNING WORKLOAD: 0+0[0m
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m context: 0-31-0-{'MLFLOW_EXPERIMENT_ID': '0', 'CUDA_VISIBLE_DEVICES': 'GPU-a5dc6d38-83c6-699e-b520-388da8593f59', 'RADT_DCGMI_GROUP': '7', 'SMI_GPU_ID': '0', 'RADT_MAX_EPOCH': '5', 'RADT_MAX_TIME': '172800', 'RADT_MANUAL_MODE': 'True', 'ps': 'False', 'smi': 'False', 'dcgmi': 'False', 'top': 'False', 'iostat': 'False', 'free': 'False', 'RADT_LISTENER_SMI': 'True', 'RADT_LISTENER_TOP': 'True', 'RADT_LISTENER_DCGMI': 'True', 'RADT_LISTENER_IOSTAT': 'True', 'RADT_LISTENER_FREE': 'True'}-['mlflow', 'run', '/home/nikch/data-selection-framework', '--env-manager=local', '-P', 'letter=0', '-P', 'workload=0', '-P', 'listeners=smi+top+dcgmi+iostat+free', '-P', 'file=tune.py', '-P', 'params="-"', '-P', 'workload_listener=']-/home/nikch/data-selection-framework
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m 2025/03/31 13:12:35 INFO mlflow.projects.utils: === Created directory /tmp/tmp0p_cegii for downloading remote URIs passed to arguments of type 'path' ===
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m 2025/03/31 13:12:35 INFO mlflow.projects.backend.local: === Running command 'python -m radt run -l smi+top+dcgmi+iostat+free -c tune.py -p "run recipe/test_lora_finetune.py --config less/warmup_train.yaml"
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m ' in run with ID '856a57a2851e4d6c88d6e39ec4ac5b54' === 
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m MAPPED TO 856a57a2851e4d6c88d6e39ec4ac5b54
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m Running LoRAFinetuneRecipeSingleDevice with resolved config:
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m 
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m batch_size: 1
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m checkpointer:
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m   _component_: torchtune.training.FullModelHFCheckpointer
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m   checkpoint_dir: model_cache/downloaded_models/Llama-3.2-1B-Instruct/
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m   checkpoint_files:
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m   - model.safetensors
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m   model_type: LLAMA3_2
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m   output_dir: model_cache/less/warmup_models/llama3_2_1B
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m   recipe_checkpoint: null
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m clip_grad_norm: null
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m compile: false
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m dataset:
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m   _component_: torchtune.datasets.alpaca_cleaned_dataset
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m   packed: false
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m device: cuda
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m dtype: bf16
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m enable_activation_checkpointing: true
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m enable_activation_offloading: true
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m epochs: 1
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m gradient_accumulation_steps: 32
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m log_every_n_steps: 1
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m log_peak_memory_stats: true
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m loss:
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m   _component_: torchtune.modules.loss.CEWithChunkedOutputLoss
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m lr_scheduler:
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m   _component_: torchtune.training.lr_schedulers.get_cosine_schedule_with_warmup
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m   num_warmup_steps: 100
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m max_steps_per_epoch: null
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m metric_logger:
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m   _component_: torchtune.training.metric_logging.DiskLogger
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m   log_dir: model_cache/less/warmup_models/llama3_2_1B/logs
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m model:
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m   _component_: torchtune.models.llama3_2.lora_llama3_2_1b
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m   apply_lora_to_mlp: true
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m   lora_alpha: 512
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m   lora_attn_modules:
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m   - q_proj
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m   - k_proj
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m   - v_proj
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m   - output_proj
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m   lora_dropout: 0.1
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m   lora_rank: 128
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m optimizer:
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m   _component_: torch.optim.AdamW
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m   fused: true
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m   lr: 2.0e-05
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m   weight_decay: 0.0
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m output_dir: model_cache/less/warmup_models/llama3_2_1B
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m profiler:
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m   _component_: torchtune.training.setup_torch_profiler
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m   active_steps: 2
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m   cpu: true
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m   cuda: true
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m   enabled: false
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m   num_cycles: 1
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m   output_dir: model_cache/less/warmup_models/llama3_2_1B/profiling_outputs
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m   profile_memory: false
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m   record_shapes: true
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m   wait_steps: 5
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m   warmup_steps: 3
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m   with_flops: false
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m   with_stack: false
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m resume_from_checkpoint: false
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m sampler:
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m   _component_: selection.PercentageBasedSampler
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m   percentage: 0.05
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m save_adapter_weights_only: false
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m seed: null
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m shuffle: true
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m tokenizer:
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m   _component_: torchtune.models.llama3.llama3_tokenizer
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m   max_seq_len: null
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m   path: model_cache/downloaded_models/Llama-3.2-1B-Instruct/original/tokenizer.model
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m 
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m Setting manual seed to local seed 2123167484. Local seed is seed + rank = 2123167484 + 0
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m Model is initialized with precision torch.bfloat16.
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m Memory stats after model init:
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m 	GPU peak memory allocation: 2.50 GiB
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m 	GPU peak memory reserved: 2.52 GiB
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m 	GPU peak memory active: 2.50 GiB
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m Tokenizer is initialized from file.
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m Optimizer and loss are initialized.
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m Loss is initialized.
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m Dataset and SelectiveSampler are initialized.
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m Learning rate scheduler is initialized.
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m  Profiling disabled.
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m  Profiler config after instantiation: {'enabled': False}
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m Failed to log parameter: params run recipe/test_lora_finetune.py --config less/warmup_train.yaml
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m Starting test_lora_finetune setup()
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m Writing logs to model_cache/less/warmup_models/llama3_2_1B/logs/log_1743419566.txt
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m starting test_lora_finetune train()
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m 2025/03/31 13:12:56 INFO mlflow.bedrock: Enabled auto-tracing for Bedrock. Note that MLflow can only trace boto3 service clients that are created after this call. If you have already created one, please recreate the client by calling `boto3.client`.
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m 2025/03/31 13:12:56 INFO mlflow.tracking.fluent: Autologging successfully enabled for boto3.
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m Process IOstatThread-4:
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m Traceback (most recent call last):
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m   File "/home/nikch/.conda/envs/selection/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m     self.run()
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m   File "/home/nikch/.conda/envs/selection/lib/python3.12/site-packages/radt/run/listeners/iostat_listener.py", line 17, in run
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m     ps = subprocess.Popen(
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m          ^^^^^^^^^^^^^^^^^
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m   File "/home/nikch/.conda/envs/selection/lib/python3.12/subprocess.py", line 1028, in __init__
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m     self._execute_child(args, executable, preexec_fn, close_fds,
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m   File "/home/nikch/.conda/envs/selection/lib/python3.12/subprocess.py", line 1963, in _execute_child
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m     raise child_exception_type(errno_num, err_msg, err_filename)
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m FileNotFoundError: [Errno 2] No such file or directory: 'iostat'
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m 
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m   0%|          | 0/80 [00:00<?, ?it/s]
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m   1%|â–         | 1/80 [00:20<27:02, 20.54s/it]
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m 1|1|Loss: 1.8919463157653809:   1%|â–         | 1/80 [00:20<27:02, 20.54s/it]
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m 1|1|Loss: 1.8919463157653809:   2%|â–Ž         | 2/80 [00:40<26:35, 20.46s/it]
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m 1|2|Loss: 2.0356626510620117:   2%|â–Ž         | 2/80 [00:40<26:35, 20.46s/it]
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m 1|2|Loss: 2.0356626510620117:   4%|â–         | 3/80 [01:01<26:23, 20.57s/it]
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m 1|3|Loss: 2.032113790512085:   4%|â–         | 3/80 [01:01<26:23, 20.57s/it] 
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m 1|3|Loss: 2.032113790512085:   5%|â–Œ         | 4/80 [01:22<25:57, 20.50s/it]
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m 1|4|Loss: 2.0336756706237793:   5%|â–Œ         | 4/80 [01:22<25:57, 20.50s/it]
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m 1|4|Loss: 2.0336756706237793:   6%|â–‹         | 5/80 [01:43<25:59, 20.79s/it]
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m 1|5|Loss: 1.8242665529251099:   6%|â–‹         | 5/80 [01:43<25:59, 20.79s/it]
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m 1|5|Loss: 1.8242665529251099:   8%|â–Š         | 6/80 [02:04<25:35, 20.75s/it]
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m 1|6|Loss: 2.002098321914673:   8%|â–Š         | 6/80 [02:04<25:35, 20.75s/it] 
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m 1|6|Loss: 2.002098321914673:   9%|â–‰         | 7/80 [02:24<25:14, 20.74s/it]
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m 1|7|Loss: 1.9578227996826172:   9%|â–‰         | 7/80 [02:24<25:14, 20.74s/it]
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m 1|7|Loss: 1.9578227996826172:  10%|â–ˆ         | 8/80 [02:45<24:43, 20.60s/it]
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m 1|8|Loss: 2.0841450691223145:  10%|â–ˆ         | 8/80 [02:45<24:43, 20.60s/it]
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m 1|8|Loss: 2.0841450691223145:  11%|â–ˆâ–        | 9/80 [03:05<24:28, 20.68s/it]
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m 1|9|Loss: 1.910002589225769:  11%|â–ˆâ–        | 9/80 [03:05<24:28, 20.68s/it] 
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m 1|9|Loss: 1.910002589225769:  12%|â–ˆâ–Ž        | 10/80 [03:27<24:17, 20.82s/it]
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m 1|10|Loss: 1.9379007816314697:  12%|â–ˆâ–Ž        | 10/80 [03:27<24:17, 20.82s/it]
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m 1|10|Loss: 1.9379007816314697:  14%|â–ˆâ–        | 11/80 [03:46<23:37, 20.55s/it]
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m 1|11|Loss: 1.9558051824569702:  14%|â–ˆâ–        | 11/80 [03:46<23:37, 20.55s/it]
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m 1|11|Loss: 1.9558051824569702:  15%|â–ˆâ–Œ        | 12/80 [04:07<23:16, 20.54s/it]
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m 1|12|Loss: 1.8505275249481201:  15%|â–ˆâ–Œ        | 12/80 [04:07<23:16, 20.54s/it]
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m 1|12|Loss: 1.8505275249481201:  16%|â–ˆâ–‹        | 13/80 [04:27<22:51, 20.48s/it]
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m 1|13|Loss: 1.806901216506958:  16%|â–ˆâ–‹        | 13/80 [04:27<22:51, 20.48s/it] 
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m 1|13|Loss: 1.806901216506958:  18%|â–ˆâ–Š        | 14/80 [04:48<22:30, 20.46s/it]
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m 1|14|Loss: 1.8278248310089111:  18%|â–ˆâ–Š        | 14/80 [04:48<22:30, 20.46s/it]
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m 1|14|Loss: 1.8278248310089111:  19%|â–ˆâ–‰        | 15/80 [05:08<22:04, 20.37s/it]
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m 1|15|Loss: 1.7861733436584473:  19%|â–ˆâ–‰        | 15/80 [05:08<22:04, 20.37s/it]
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m 1|15|Loss: 1.7861733436584473:  20%|â–ˆâ–ˆ        | 16/80 [05:29<21:56, 20.57s/it]
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m 1|16|Loss: 1.5870378017425537:  20%|â–ˆâ–ˆ        | 16/80 [05:29<21:56, 20.57s/it]
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m 1|16|Loss: 1.5870378017425537:  21%|â–ˆâ–ˆâ–       | 17/80 [05:50<21:42, 20.67s/it]
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m 1|17|Loss: 1.6462485790252686:  21%|â–ˆâ–ˆâ–       | 17/80 [05:50<21:42, 20.67s/it]
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m 1|17|Loss: 1.6462485790252686:  22%|â–ˆâ–ˆâ–Ž       | 18/80 [06:10<21:15, 20.58s/it]
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m 1|18|Loss: 1.6587837934494019:  22%|â–ˆâ–ˆâ–Ž       | 18/80 [06:10<21:15, 20.58s/it]
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m 1|18|Loss: 1.6587837934494019:  24%|â–ˆâ–ˆâ–       | 19/80 [06:31<20:53, 20.56s/it]
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m 1|19|Loss: 1.5423351526260376:  24%|â–ˆâ–ˆâ–       | 19/80 [06:31<20:53, 20.56s/it]
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m 1|19|Loss: 1.5423351526260376:  25%|â–ˆâ–ˆâ–Œ       | 20/80 [06:51<20:25, 20.42s/it]
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m 1|20|Loss: 1.6471500396728516:  25%|â–ˆâ–ˆâ–Œ       | 20/80 [06:51<20:25, 20.42s/it]
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m 1|20|Loss: 1.6471500396728516:  26%|â–ˆâ–ˆâ–‹       | 21/80 [07:11<19:59, 20.33s/it]
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m 1|21|Loss: 1.5338102579116821:  26%|â–ˆâ–ˆâ–‹       | 21/80 [07:11<19:59, 20.33s/it]
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m 1|21|Loss: 1.5338102579116821:  28%|â–ˆâ–ˆâ–Š       | 22/80 [07:31<19:43, 20.40s/it]
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m 1|22|Loss: 1.4700886011123657:  28%|â–ˆâ–ˆâ–Š       | 22/80 [07:31<19:43, 20.40s/it]
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m 1|22|Loss: 1.4700886011123657:  29%|â–ˆâ–ˆâ–‰       | 23/80 [07:52<19:27, 20.49s/it]
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m 1|23|Loss: 1.3837335109710693:  29%|â–ˆâ–ˆâ–‰       | 23/80 [07:52<19:27, 20.49s/it]
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m 1|23|Loss: 1.3837335109710693:  30%|â–ˆâ–ˆâ–ˆ       | 24/80 [08:12<19:02, 20.41s/it]
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m 1|24|Loss: 1.5059489011764526:  30%|â–ˆâ–ˆâ–ˆ       | 24/80 [08:12<19:02, 20.41s/it]
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m 1|24|Loss: 1.5059489011764526:  31%|â–ˆâ–ˆâ–ˆâ–      | 25/80 [08:32<18:32, 20.23s/it]
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m 1|25|Loss: 1.548826813697815:  31%|â–ˆâ–ˆâ–ˆâ–      | 25/80 [08:32<18:32, 20.23s/it] 
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m 1|25|Loss: 1.548826813697815:  32%|â–ˆâ–ˆâ–ˆâ–Ž      | 26/80 [08:53<18:17, 20.33s/it]
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m 1|26|Loss: 1.3628785610198975:  32%|â–ˆâ–ˆâ–ˆâ–Ž      | 26/80 [08:53<18:17, 20.33s/it]
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m 1|26|Loss: 1.3628785610198975:  34%|â–ˆâ–ˆâ–ˆâ–      | 27/80 [09:13<17:54, 20.27s/it]
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m 1|27|Loss: 1.3266472816467285:  34%|â–ˆâ–ˆâ–ˆâ–      | 27/80 [09:13<17:54, 20.27s/it]
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m 1|27|Loss: 1.3266472816467285:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 28/80 [09:34<17:39, 20.37s/it]
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m 1|28|Loss: 1.2885215282440186:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 28/80 [09:34<17:39, 20.37s/it]
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m 1|28|Loss: 1.2885215282440186:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 29/80 [09:53<17:11, 20.22s/it]
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m 1|29|Loss: 1.2693144083023071:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 29/80 [09:53<17:11, 20.22s/it]
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m 1|29|Loss: 1.2693144083023071:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 30/80 [10:14<17:00, 20.41s/it]
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m 1|30|Loss: 1.273329257965088:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 30/80 [10:14<17:00, 20.41s/it] 
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m 1|30|Loss: 1.273329257965088:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 31/80 [10:34<16:27, 20.15s/it]
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m 1|31|Loss: 1.1989569664001465:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 31/80 [10:34<16:27, 20.15s/it]
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m 1|31|Loss: 1.1989569664001465:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 32/80 [10:54<16:09, 20.20s/it]
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m 1|32|Loss: 1.283942461013794:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 32/80 [10:54<16:09, 20.20s/it] 
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m 1|32|Loss: 1.283942461013794:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 33/80 [11:15<15:54, 20.32s/it]
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m 1|33|Loss: 1.2643940448760986:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 33/80 [11:15<15:54, 20.32s/it]
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m 1|33|Loss: 1.2643940448760986:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 34/80 [11:35<15:36, 20.35s/it]
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m 1|34|Loss: 1.201050043106079:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 34/80 [11:35<15:36, 20.35s/it] 
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m 1|34|Loss: 1.201050043106079:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 35/80 [11:56<15:21, 20.48s/it]
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m 1|35|Loss: 1.2853732109069824:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 35/80 [11:56<15:21, 20.48s/it]
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m 1|35|Loss: 1.2853732109069824:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 36/80 [12:16<15:00, 20.47s/it]
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m 1|36|Loss: 1.2293404340744019:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 36/80 [12:16<15:00, 20.47s/it]
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m 1|36|Loss: 1.2293404340744019:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 37/80 [12:37<14:44, 20.57s/it]
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m 1|37|Loss: 1.2895123958587646:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 37/80 [12:37<14:44, 20.57s/it]
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m 1|37|Loss: 1.2895123958587646:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 38/80 [12:57<14:12, 20.30s/it]
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m 1|38|Loss: 1.263093113899231:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 38/80 [12:57<14:12, 20.30s/it] 
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m 1|38|Loss: 1.263093113899231:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 39/80 [13:18<14:00, 20.50s/it]
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m 1|39|Loss: 1.265363097190857:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 39/80 [13:18<14:00, 20.50s/it]
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m 1|39|Loss: 1.265363097190857:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 40/80 [13:38<13:34, 20.37s/it]
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m 1|40|Loss: 1.2072259187698364:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 40/80 [13:38<13:34, 20.37s/it]
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m 1|40|Loss: 1.2072259187698364:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 41/80 [13:58<13:12, 20.31s/it]
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m 1|41|Loss: 1.1850117444992065:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 41/80 [13:58<13:12, 20.31s/it]
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m 1|41|Loss: 1.1850117444992065:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 42/80 [14:19<12:57, 20.45s/it]
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m 1|42|Loss: 1.1172130107879639:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 42/80 [14:19<12:57, 20.45s/it]
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m 1|42|Loss: 1.1172130107879639:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 43/80 [14:40<12:41, 20.58s/it]
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m 1|43|Loss: 1.2274399995803833:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 43/80 [14:40<12:41, 20.58s/it]
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m 1|43|Loss: 1.2274399995803833:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 44/80 [15:00<12:16, 20.47s/it]
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m 1|44|Loss: 1.2147140502929688:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 44/80 [15:00<12:16, 20.47s/it]
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m 1|44|Loss: 1.2147140502929688:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 45/80 [15:21<11:59, 20.57s/it]
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m 1|45|Loss: 1.0817381143569946:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 45/80 [15:21<11:59, 20.57s/it]
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m 1|45|Loss: 1.0817381143569946:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 46/80 [15:42<11:44, 20.73s/it]
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m 1|46|Loss: 1.2018375396728516:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 46/80 [15:42<11:44, 20.73s/it]
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m 1|46|Loss: 1.2018375396728516:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 47/80 [16:02<11:20, 20.61s/it]
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m 1|47|Loss: 1.225875735282898:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 47/80 [16:02<11:20, 20.61s/it] 
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m 1|47|Loss: 1.225875735282898:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 48/80 [16:23<10:59, 20.60s/it]
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m 1|48|Loss: 1.1951355934143066:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 48/80 [16:23<10:59, 20.60s/it]
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m 1|48|Loss: 1.1951355934143066:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 49/80 [16:43<10:32, 20.39s/it]
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m 1|49|Loss: 1.1483843326568604:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 49/80 [16:43<10:32, 20.39s/it]
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m 1|49|Loss: 1.1483843326568604:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 50/80 [17:03<10:08, 20.30s/it]
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m 1|50|Loss: 1.2997410297393799:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 50/80 [17:03<10:08, 20.30s/it]
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m 1|50|Loss: 1.2997410297393799:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 51/80 [17:23<09:46, 20.23s/it]
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m 1|51|Loss: 1.2909280061721802:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 51/80 [17:23<09:46, 20.23s/it]
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m 1|51|Loss: 1.2909280061721802:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 52/80 [17:42<09:21, 20.07s/it]
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m 1|52|Loss: 1.169724702835083:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 52/80 [17:42<09:21, 20.07s/it] 
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m 1|52|Loss: 1.169724702835083:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 53/80 [18:02<09:01, 20.06s/it]
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m 1|53|Loss: 1.213497519493103:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 53/80 [18:02<09:01, 20.06s/it]
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m 1|53|Loss: 1.213497519493103:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 54/80 [18:23<08:48, 20.33s/it]
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m 1|54|Loss: 1.1744041442871094:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 54/80 [18:23<08:48, 20.33s/it]
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m 1|54|Loss: 1.1744041442871094:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 55/80 [18:44<08:28, 20.36s/it]
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m 1|55|Loss: 1.1038215160369873:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 55/80 [18:44<08:28, 20.36s/it]
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m 1|55|Loss: 1.1038215160369873:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 56/80 [19:04<08:07, 20.31s/it]
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m 1|56|Loss: 1.2023791074752808:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 56/80 [19:04<08:07, 20.31s/it]
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m 1|56|Loss: 1.2023791074752808:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 57/80 [19:25<07:50, 20.44s/it]
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m 1|57|Loss: 1.2300790548324585:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 57/80 [19:25<07:50, 20.44s/it]
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m 1|57|Loss: 1.2300790548324585:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 58/80 [19:45<07:26, 20.29s/it]
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m 1|58|Loss: 1.1236896514892578:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 58/80 [19:45<07:26, 20.29s/it]
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m 1|58|Loss: 1.1236896514892578:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 59/80 [20:05<07:03, 20.19s/it]
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m 1|59|Loss: 1.1827892065048218:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 59/80 [20:05<07:03, 20.19s/it]
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m 1|59|Loss: 1.1827892065048218:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 60/80 [20:25<06:43, 20.17s/it]
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m 1|60|Loss: 1.0912129878997803:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 60/80 [20:25<06:43, 20.17s/it]
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m 1|60|Loss: 1.0912129878997803:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 61/80 [20:45<06:25, 20.30s/it]
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m 1|61|Loss: 1.1405141353607178:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 61/80 [20:45<06:25, 20.30s/it]
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m 1|61|Loss: 1.1405141353607178:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 62/80 [21:06<06:06, 20.36s/it]
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m 1|62|Loss: 1.2222644090652466:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 62/80 [21:06<06:06, 20.36s/it]
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m 1|62|Loss: 1.2222644090652466:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 63/80 [21:26<05:46, 20.36s/it]
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m 1|63|Loss: 1.2503339052200317:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 63/80 [21:26<05:46, 20.36s/it]
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m 1|63|Loss: 1.2503339052200317:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 64/80 [21:46<05:24, 20.29s/it]
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m 1|64|Loss: 1.337704062461853:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 64/80 [21:46<05:24, 20.29s/it] 
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m 1|64|Loss: 1.337704062461853:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 65/80 [22:07<05:03, 20.24s/it]
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m 1|65|Loss: 1.1321296691894531:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 65/80 [22:07<05:03, 20.24s/it]
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m 1|65|Loss: 1.1321296691894531:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 66/80 [22:26<04:42, 20.15s/it]
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m 1|66|Loss: 1.2442803382873535:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 66/80 [22:26<04:42, 20.15s/it]
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m 1|66|Loss: 1.2442803382873535:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 67/80 [22:46<04:20, 20.06s/it]
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m 1|67|Loss: 1.147700548171997:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 67/80 [22:46<04:20, 20.06s/it] 
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m 1|67|Loss: 1.147700548171997:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 68/80 [23:06<04:00, 20.05s/it]
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m 1|68|Loss: 1.2002006769180298:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 68/80 [23:06<04:00, 20.05s/it]
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m 1|68|Loss: 1.2002006769180298:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 69/80 [23:27<03:42, 20.27s/it]
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m 1|69|Loss: 1.3239858150482178:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 69/80 [23:27<03:42, 20.27s/it]
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m 1|69|Loss: 1.3239858150482178:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 70/80 [23:47<03:22, 20.29s/it]
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m 1|70|Loss: 1.1693329811096191:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 70/80 [23:47<03:22, 20.29s/it]
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m 1|70|Loss: 1.1693329811096191:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 71/80 [24:08<03:03, 20.40s/it]
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m 1|71|Loss: 1.232743263244629:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 71/80 [24:08<03:03, 20.40s/it] 
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m 1|71|Loss: 1.232743263244629:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 72/80 [24:29<02:43, 20.39s/it]
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m 1|72|Loss: 1.1999523639678955:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 72/80 [24:29<02:43, 20.39s/it]
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m 1|72|Loss: 1.1999523639678955:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 73/80 [24:48<02:21, 20.24s/it]
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m 1|73|Loss: 1.155151128768921:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 73/80 [24:48<02:21, 20.24s/it] 
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m 1|73|Loss: 1.155151128768921:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 74/80 [25:08<01:59, 19.99s/it]
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m 1|74|Loss: 1.2496000528335571:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 74/80 [25:08<01:59, 19.99s/it]
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m 1|74|Loss: 1.2496000528335571:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 75/80 [25:28<01:40, 20.14s/it]
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m 1|75|Loss: 1.1252449750900269:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 75/80 [25:28<01:40, 20.14s/it]
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m 1|75|Loss: 1.1252449750900269:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 76/80 [25:49<01:21, 20.42s/it]
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m 1|76|Loss: 1.0942847728729248:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 76/80 [25:49<01:21, 20.42s/it]
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m 1|76|Loss: 1.0942847728729248:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 77/80 [26:10<01:01, 20.42s/it]
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m 1|77|Loss: 1.1527206897735596:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 77/80 [26:10<01:01, 20.42s/it]
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m 1|77|Loss: 1.1527206897735596:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 78/80 [26:30<00:40, 20.35s/it]
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m 1|78|Loss: 1.1117058992385864:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 78/80 [26:30<00:40, 20.35s/it]
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m 1|78|Loss: 1.1117058992385864:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 79/80 [26:50<00:20, 20.31s/it]
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m 1|79|Loss: 1.133478045463562:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 79/80 [26:50<00:20, 20.31s/it] 
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m 1|79|Loss: 1.133478045463562: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 80/80 [27:10<00:00, 20.21s/it]
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m 1|80|Loss: 1.1160051822662354: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 80/80 [27:10<00:00, 20.21s/it]Starting checkpoint save...
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m [31m2025/03/31 13:41:19 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.[0m
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m 2025/03/31 13:41:57 ERROR mlflow.cli: === Run (ID '856a57a2851e4d6c88d6e39ec4ac5b54') failed ===
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m ðŸƒ View run LESS_WarmUp_Test at: https://res17.itu.dk/#/experiments/0/runs/856a57a2851e4d6c88d6e39ec4ac5b54
[31m[RUN 0]:            [0m [31m[RUN 0]:            [0m ðŸ§ª View experiment at: https://res17.itu.dk/#/experiments/0
slurmstepd: error: *** JOB 40939 ON cn3 CANCELLED AT 2025-03-31T16:53:17 ***
slurmstepd: error: Detected 1 oom_kill event in StepId=40939.batch. Some of the step tasks have been OOM Killed.
