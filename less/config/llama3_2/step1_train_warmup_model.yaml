model_name: llama3_2_1B_instruct
output_dir: model_cache/less/warmup_models/${model_name}

sampler:
  _component_: selection.PercentageBasedSampler
  percentage: 0.05 # How much data is selected for model warmup, # Remember to change lr_scheduler.num_warmup_steps accordingly

# Model Arguments
model:
  _component_: torchtune.models.llama3_2.lora_llama3_2_1b
  lora_attn_modules: ['q_proj', 'k_proj', 'v_proj', 'output_proj'] # LESS default = q_proj k_proj v_proj output_proj
  apply_lora_to_mlp: True # LESS default = True
  lora_rank: 128  # LESS default = 128
  lora_alpha: 512  # LESS default = 512
  lora_dropout: 0.1 # LESS default = 0.1

# Tokenizer
tokenizer:
  _component_: torchtune.models.llama3.llama3_tokenizer
  path: model_cache/downloaded_models/Llama-3.2-1B-Instruct/original/tokenizer.model
  max_seq_len: 2048 # LESS default = 2048

checkpointer:
  _component_: torchtune.training.FullModelHFCheckpointer
  checkpoint_dir: model_cache/downloaded_models/Llama-3.2-1B-Instruct/
  checkpoint_files: [
     model.safetensors
  ]
  recipe_checkpoint: null
  output_dir: ${output_dir}
  model_type: LLAMA3_2
resume_from_checkpoint: False
save_adapter_weights_only: False

# Dataset and Sampler
dataset:
  _component_: torchtune.datasets.alpaca_cleaned_dataset
  packed: False  # True increases speed
seed: 42
shuffle: True
same_shuffle_every_epoch: True # LESS use the same selection (percentage) of data samples in every epoch. This ensures we use the same data samples (the same shuffle seed) every epoch with the percentageSampler. However LESS shuffle this selected percentage differently inside every epoch
n_print_examples: 3
batch_size: 8 # LESS default = 1

# Optimizer and Scheduler
optimizer:
  _component_: torch.optim.AdamW
  fused: True
  weight_decay: 0.0 # LESS default = 0.0
  lr: 2e-05 # LESS default = 2e-05
lr_scheduler:
  _component_: torchtune.training.lr_schedulers.get_cosine_schedule_with_warmup # LESS default use linear warmup, cosine decay
  num_warmup_steps: 100 # LESS default use a warmup_ratio of 0.03, you will need to calculate the equivalent manually.
  # num_warmup_steps should be 0.03 * (n_selected_train_data_samples / batch_size * gradient_accumulation_steps)


loss:
  _component_: torchtune.modules.loss.CEWithChunkedOutputLoss

# Training
epochs: 4 # LESS default = 4
max_steps_per_epoch: null # LESS default = unknown
gradient_accumulation_steps: 4  # Use to increase effective batch size # LESS default = 32
clip_grad_norm: null
compile: False  # torch.compile the model + loss, True increases speed + decreases memory

# Logging
metric_logger:
  _component_: torchtune.training.metric_logging.DiskLogger
  log_dir: ${output_dir}/logs
log_every_n_steps: 1
log_peak_memory_stats: True

# Environment
device: cuda
dtype: bf16 # LESS default = "bf16 True"

# Activations Memory
enable_activation_checkpointing: False  # True reduces memory
enable_activation_offloading: False  # True reduces memory


# Profiler (disabled)
profiler:
  _component_: torchtune.training.setup_torch_profiler
  enabled: False

  #Output directory of trace artifacts
  output_dir: ${output_dir}/profiling_outputs

  #`torch.profiler.ProfilerActivity` types to trace
  cpu: True
  cuda: True

  #trace options passed to `torch.profiler.profile`
  profile_memory: False
  with_stack: False
  record_shapes: True
  with_flops: False

  # `torch.profiler.schedule` options:
  # wait_steps -> wait, warmup_steps -> warmup, active_steps -> active, num_cycles -> repeat
  wait_steps: 5
  warmup_steps: 3
  active_steps: 2
  num_cycles: 1
